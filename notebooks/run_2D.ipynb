{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "490f81c7",
   "metadata": {},
   "source": [
    "## 2D PIPELINE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82bcd71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/09 02:04:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - [INFO] Initializing models on device: cuda:0\n",
      "Loads checkpoint by local backend from path: ./model/detection/detection.pth\n",
      "Loads checkpoint by local backend from path: ./model/pose/pose.pth\n",
      "Loads checkpoint by local backend from path: ./model/id/id_finetuned.pth\n",
      "06/09 02:04:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - [INFO] All models initialized.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/mnt/nas_siddharth/code_final/')\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.pipeline.step1_proc2d import init_all_models\n",
    "from mmcv.transforms import Compose\n",
    "\n",
    "DEVICE = 'cuda:0'\n",
    "\n",
    "detector, tracker, pose_model, test_pipeline, id_model_all = init_all_models(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b026369",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipeline = Compose([\n",
    "    {\"type\": \"mmdet.LoadImageFromNDArray\"},\n",
    "    {\"type\": \"Resize\", \"scale\": (800, 800), \"keep_ratio\": True},\n",
    "    {\"type\": \"mmdet.LoadAnnotations\", \"with_bbox\": True},\n",
    "    {\"type\": \"mmdet.PackDetInputs\"},\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f96e81e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = '/mnt/nas_siddharth/code_final/notebooks/video/test_video.mp4'\n",
    "output_path = '/mnt/nas_siddharth/code_final/notebooks/video/test_video_inference.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc9289ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton_pairs = [\n",
    "    (0, 1),   # nose - left eye\n",
    "    (0, 2),   # nose - right eye\n",
    "    (1, 3),   # left eye - left ear\n",
    "    (2, 4),   # right eye - right ear\n",
    "    (3, 4),   # left ear - right ear\n",
    "    #(0, 17),  # nose - neck\n",
    "    (3, 17),  # left ear - neck\n",
    "    (4, 17),  # right ear - neck\n",
    "    (17, 5),  # neck - left shoulder\n",
    "    (17, 6),  # neck - right shoulder\n",
    "    (5, 6),   # left shoulder - right shoulder\n",
    "    (17, 11), # neck - left hip\n",
    "    (17, 12), # neck - right hip\n",
    "    (11, 12), # left hip - right hip\n",
    "    (5, 7),   # left shoulder - left elbow\n",
    "    (7, 9),   # left elbow - left wrist\n",
    "    (6, 8),   # right shoulder - right elbow\n",
    "    (8, 10),  # right elbow - right wrist\n",
    "    (11, 13), # left hip - left knee\n",
    "    (13, 15), # left knee - left ankle\n",
    "    (12, 14), # right hip - right knee\n",
    "    (14, 16), # right knee - right ankle\n",
    "    # Diagonals for torso\n",
    "    #(5, 11),  # left shoulder - left hip\n",
    "    #(6, 12),  # right shoulder - right hip\n",
    "    #(5, 12),  # left shoulder - right hip\n",
    "    #(6, 11),  # right shoulder - left hip\n",
    "    ]\n",
    "\n",
    "def draw_kps_cv2(img, kpts, skeleton_pairs, clr=(0,255,0)):\n",
    "    for i, (x, y, score) in enumerate(kpts):\n",
    "        if score > 0.2:\n",
    "            cv2.circle(img, (int(x), int(y)), 2, clr, -1)\n",
    "    for i1, i2 in skeleton_pairs:\n",
    "        if kpts[i1][2]>0.2 and kpts[i2][2]>0.2:\n",
    "            pt1 = (int(kpts[i1][0]), int(kpts[i1][1]))\n",
    "            pt2 = (int(kpts[i2][0]), int(kpts[i2][1]))\n",
    "            cv2.line(img, pt1, pt2, clr, 1)\n",
    "    return img\n",
    "\n",
    "def detect_and_pose_on_image(img, detector, pose_model, test_pipeline):\n",
    "    from mmdet.apis import inference_detector\n",
    "    det_result = inference_detector(detector, [img], test_pipeline=test_pipeline)[0]\n",
    "    bboxes = det_result.pred_instances.bboxes.cpu().numpy()\n",
    "    scores = det_result.pred_instances.scores.cpu().numpy()\n",
    "    score_thr = 0.3\n",
    "    keep = scores > score_thr\n",
    "    bboxes = bboxes[keep]\n",
    "    person_results = [{\"bbox\": bbox} for bbox in bboxes]\n",
    "    from mmpose.apis import inference_topdown\n",
    "    if len(person_results) > 0:\n",
    "        pose_results = inference_topdown(\n",
    "            pose_model, img,\n",
    "            bboxes=np.array([r[\"bbox\"] for r in person_results], dtype=np.float32),\n",
    "            bbox_format=\"xyxy\",\n",
    "        )\n",
    "    else:\n",
    "        pose_results = []\n",
    "    return pose_results\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "w, h = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "nframes = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "save_out = True\n",
    "if save_out:\n",
    "    out_writer = cv2.VideoWriter(\n",
    "        output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5d6957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_sec = 44\n",
    "end_sec   = 54\n",
    "\n",
    "fps = 24\n",
    "start_frame = int(start_sec * fps)\n",
    "end_frame = int(end_sec * fps)\n",
    "nframes = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "end_frame = min(end_frame, nframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57fde011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 25.0, total frames: 10000\n",
      "44s to 54s (frames 1056 to 1296)\n"
     ]
    }
   ],
   "source": [
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "nframes = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(f\"FPS: {fps}, total frames: {nframes}\")\n",
    "print(f\"{start_sec}s to {end_sec}s (frames {start_frame} to {end_frame})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a4d061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing selected frames:   0%|          | 0/240 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/09 02:05:34 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The current default scope \"mmpretrain\" is not \"mmpose\", `init_default_scope` will force set the currentdefault scope to \"mmpose\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing selected frames: 100%|██████████| 240/240 [01:08<00:00,  3.52it/s]\n"
     ]
    }
   ],
   "source": [
    "sample_frames = []\n",
    "frame_idxs_to_show = [0, nframes//2, nframes-1]  # first, middle, last\n",
    "\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "for i in tqdm(range(start_frame, end_frame), desc=\"Processing selected frames\"):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img_vis = frame.copy()\n",
    "    pose_results = detect_and_pose_on_image(frame, detector, pose_model, test_pipeline)\n",
    "    for res in pose_results:\n",
    "        kpts = res.pred_instances.keypoints[0]\n",
    "        scores = res.pred_instances.keypoint_scores[0]\n",
    "        if not (np.isnan(kpts[5]).any() or np.isnan(kpts[6]).any()):\n",
    "            neck_xy = (kpts[5] + kpts[6]) / 2.0\n",
    "            neck_score = (scores[5] + scores[6]) / 2.0\n",
    "        else:\n",
    "            neck_xy = np.array([np.nan, np.nan])\n",
    "            neck_score = 0.0\n",
    "        kpts18 = np.vstack([kpts, neck_xy])\n",
    "        scores18 = np.concatenate([scores, [neck_score]])\n",
    "        kpts_vis = np.concatenate([kpts18, scores18[:, None]], axis=1)\n",
    "        img_vis = draw_kps_cv2(img_vis, kpts_vis, skeleton_pairs)\n",
    "\n",
    "    if save_out:\n",
    "        out_writer.write(img_vis)\n",
    "    if i in frame_idxs_to_show:\n",
    "        sample_frames.append(cv2.cvtColor(img_vis, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "cap.release()\n",
    "if save_out:\n",
    "    out_writer.release()\n",
    "\n",
    "for idx, img_rgb in zip(frame_idxs_to_show, sample_frames):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title(f\"Frame {idx}\")\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
